{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths using raw strings for clarity\n",
    "text_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\dataset\\text.csv'\n",
    "emotion_stimulus_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\dataset\\emotion-stimulus.csv'\n",
    "dailydialog_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\dataset\\dailydialog.csv'\n",
    "emotion_sentimen_dataset_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\dataset\\emotion_sentimen_dataset.csv'\n",
    "\n",
    "sampled_text_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\Subdataset\\sampled_text.csv'\n",
    "sampled_dailydialog_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\Subdataset\\sampled_dailydialog.csv'\n",
    "sampled_emotion_sentimen_dataset_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\Subdataset\\sampled_emotion_sentimen_dataset_path.csv'\n",
    "\n",
    "combined_data_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\Subdataset\\combined_dataset.csv'\n",
    "combined_data_path2 = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\Subdataset\\combined_dataset2.csv'\n",
    "final_combined_data_path = r'C:\\Users\\wangshuqi\\Desktop\\ML_Group1\\Tweets\\Group1\\Data\\Subdataset\\final_combined_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Text.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(text_path)\n",
    "\n",
    "# Define the labels to extract and the number of samples for each label\n",
    "selected_labels = [0, 1, 3, 4, 5]\n",
    "samples_per_label = {0: 9000, 1: 3000, 3: 11000, 4: 18100, 5: 20000}  # Adjusted number for 'fear'\n",
    "\n",
    "# Check if each label has enough samples\n",
    "counts = data['label'].value_counts()\n",
    "sufficient_data = all(counts.get(label, 0) >= samples_per_label[label] for label in selected_labels)\n",
    "\n",
    "# Extract data based on labels\n",
    "if sufficient_data:\n",
    "    sampled_data = pd.concat([data[data['label'] == label].sample(samples_per_label[label], random_state=1) for label in selected_labels])\n",
    "else:\n",
    "    sampled_data = pd.concat([data[data['label'] == label].sample(min(counts.get(label, 0), samples_per_label[label]), random_state=1) for label in selected_labels])\n",
    "\n",
    "# Replace numeric labels with emotion descriptors\n",
    "emotion_map = {0: 'sadness', 1: 'joy', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
    "sampled_data['label'] = sampled_data['label'].map(emotion_map)\n",
    "\n",
    "# Keep only the 'label' and 'text' columns, and adjust the order\n",
    "sampled_data = sampled_data[['label', 'text']]\n",
    "\n",
    "# Save the extracted data to a new CSV file\n",
    "sampled_data.to_csv(sampled_text_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with emotion-stimulus.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the emotion-stimulus dataset\n",
    "emotion_data = pd.read_csv(emotion_stimulus_path)\n",
    "\n",
    "# Filter the emotion-stimulus dataset to include only specified emotions\n",
    "required_emotions = ['happy', 'sad', 'anger', 'fear', 'surprise']\n",
    "emotion_data = emotion_data[emotion_data['Emotion'].isin(required_emotions)]\n",
    "\n",
    "# Replace the 'Emotion' column labels to match the existing labels\n",
    "emotion_map_new = {\n",
    "    'happy': 'happiness',\n",
    "    'sad': 'sadness',\n",
    "    'anger': 'anger',\n",
    "    'fear': 'fear',\n",
    "    'surprise': 'surprise'\n",
    "}\n",
    "emotion_data['Emotion'] = emotion_data['Emotion'].map(emotion_map_new)\n",
    "\n",
    "# Rename columns to match the combined dataset format\n",
    "emotion_data.rename(columns={'Emotion': 'label', 'Text': 'text'}, inplace=True)\n",
    "\n",
    "# Load the previously created sampled dataset\n",
    "sampled_text_data = pd.read_csv(sampled_text_path)\n",
    "\n",
    "# Concatenate the balanced dataset with the filtered emotion-stimulus dataset\n",
    "combined_data = pd.concat([sampled_text_data, emotion_data], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset to a new CSV file\n",
    "combined_data.to_csv(combined_data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling dailydialog.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dailydialog_path)\n",
    "\n",
    "# Define the labels to extract and their corresponding emotion descriptors\n",
    "label_map = {\n",
    "    0: 'neutral',\n",
    "    1: 'anger',\n",
    "    3: 'fear',\n",
    "    4: 'joy',  \n",
    "    5: 'sadness',\n",
    "    6: 'surprise'\n",
    "}\n",
    "selected_labels = list(label_map.keys())\n",
    "samples_per_label = 13000\n",
    "\n",
    "# Filter and sample data based on specified labels\n",
    "sampled_data = pd.DataFrame()\n",
    "for label in selected_labels:\n",
    "    filtered_data = data[data['Emotion'] == label]\n",
    "    if len(filtered_data) >= samples_per_label:\n",
    "        sampled = filtered_data.sample(n=samples_per_label, random_state=1)\n",
    "    else:\n",
    "        sampled = filtered_data\n",
    "    sampled_data = pd.concat([sampled_data, sampled])\n",
    "\n",
    "# Map the numerical labels to emotion descriptors\n",
    "sampled_data['Emotion'] = sampled_data['Emotion'].map(label_map)\n",
    "\n",
    "# Rename columns to match the expected format\n",
    "sampled_data.rename(columns={'Emotion': 'label', 'Text': 'text'}, inplace=True)\n",
    "\n",
    "# Save the extracted and mapped data to a new CSV file\n",
    "sampled_data.to_csv(sampled_dailydialog_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with sampled dailydialog.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "combined_data = pd.read_csv(combined_data_path)\n",
    "dailydialog_data = pd.read_csv(sampled_dailydialog_path)\n",
    "\n",
    "# Concatenate the two datasets\n",
    "combined_data2 = pd.concat([combined_data, dailydialog_data], ignore_index=True)\n",
    "\n",
    "# Save the final combined dataset to a new CSV file\n",
    "combined_data2.to_csv(combined_data_path2, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling emotion_sentimen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangshuqi\\AppData\\Local\\Temp\\ipykernel_11632\\1347787071.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['label'] = data_filtered['label'].replace('fun', 'joy')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(emotion_sentimen_dataset_path)\n",
    "\n",
    "# Remove the first column\n",
    "data_cleaned = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "# Swap the 'text' and 'Emotion' columns\n",
    "data_cleaned = data_cleaned[['Emotion', 'text']]\n",
    "\n",
    "# Rename the columns\n",
    "data_cleaned.columns = ['label', 'text']\n",
    "\n",
    "# Filter out rows where 'label' is 'neutral', 'boredom', or 'empty'\n",
    "labels_to_remove = ['neutral', 'boredom', 'empty']\n",
    "data_filtered = data_cleaned[~data_cleaned['label'].isin(labels_to_remove)]\n",
    "\n",
    "# Replace 'fun' with 'joy'\n",
    "data_filtered['label'] = data_filtered['label'].replace('fun', 'joy')\n",
    "\n",
    "# Save the filtered dataset to a new CSV file\n",
    "data_filtered.to_csv(sampled_emotion_sentimen_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "combined_data2 = pd.read_csv(combined_data_path2)\n",
    "sampled_emotion_sentimen_dataset = pd.read_csv(sampled_emotion_sentimen_dataset_path)\n",
    "\n",
    "# Concatenate the two datasets\n",
    "final_combined_data = pd.concat([combined_data2,sampled_emotion_sentimen_dataset], ignore_index=True)\n",
    "\n",
    "# List of emotions to remove\n",
    "emotions_to_remove = ['worry', 'relief', 'enthusiasm', 'hate']\n",
    "\n",
    "# Filter out rows with these emotions\n",
    "final_combined_data = final_combined_data[~final_combined_data['label'].isin(emotions_to_remove)]\n",
    "\n",
    "# Save the final combined dataset to a new CSV file\n",
    "final_combined_data.to_csv(final_combined_data_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(final_combined_data_path)\n",
    "\n",
    "# remove duplicates\n",
    "data_cleaned = data.drop_duplicates()\n",
    "\n",
    "# Split the data into training and remaining data with 80% for training\n",
    "train_data, remaining_data = train_test_split(data_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test data equally\n",
    "validation_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the datasets to CSV files\n",
    "train_data.to_csv('C:\\\\Users\\\\wangshuqi\\\\Desktop\\\\ML_Group1\\\\Tweets\\\\Group1\\\\Data\\\\train_data.csv', index=False)\n",
    "validation_data.to_csv('C:\\\\Users\\\\wangshuqi\\\\Desktop\\\\ML_Group1\\\\Tweets\\\\Group1\\\\Data\\\\validation_data.csv', index=False)\n",
    "test_data.to_csv('C:\\\\Users\\\\wangshuqi\\\\Desktop\\\\ML_Group1\\\\Tweets\\\\Group1\\\\Data\\\\test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
